{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical\n"
      ],
      "metadata": {
        "id": "_psFNZhtwdDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Can we use Bagging for regression problem.\n",
        "- Yes, Bagging can be used for regression problems (Bagging Regressor) as well as classification problems (Bagging Classifier)."
      ],
      "metadata": {
        "id": "X2rqUXFpwdLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training.\n",
        "- Single model training: One model is trained on the dataset.\n",
        "\n",
        "- Multiple model training (Ensemble): Multiple models are trained and combined to improve performance and robustness."
      ],
      "metadata": {
        "id": "zST04IilwdQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "- Random Forest introduces randomness by selecting a random subset of features at each split, which helps in reducing correlation between individual trees."
      ],
      "metadata": {
        "id": "Vb2z3yzZwdTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score\n",
        "- OOB Score is an internal validation score computed from data samples not included in the bootstrap sample for a given tree."
      ],
      "metadata": {
        "id": "5avtb90HwdW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How can you measure the importance of features in a Random Forest model\n",
        "- Feature importance can be measured by calculating how much each feature decreases impurity across the forest or using permutation importance."
      ],
      "metadata": {
        "id": "M8UQPfQMwdaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the working principle of a Bagging Classifier.\n",
        "- Bagging builds multiple versions of a predictor (classifier) using bootstrap samples and combines their predictions (majority vote for classification)."
      ],
      "metadata": {
        "id": "oQCrtjLhwdhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifierâ€™s performance.\n",
        "- You can use accuracy, precision, recall, F1-score, confusion matrix, cross-validation, and ROC-AUC depending on the problem.\n"
      ],
      "metadata": {
        "id": "hpMkL6V0wdkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  How does a Bagging Regressor work.\n",
        "- Similar to classifier, but predictions are averaged instead of voting."
      ],
      "metadata": {
        "id": "7UAMwMZywdoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques.\n",
        "- They reduce variance and overfitting, improving accuracy and robustness.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Zr-Qa0EwdsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods\n",
        "- They can be computationally expensive, difficult to interpret, and require careful tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "INDaVUoNwdwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Explain the key idea behind ensemble techniques.\n",
        "- Combine multiple weak learners to create a strong learner with better generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "6nyD0zGOwdy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier\n",
        "- An ensemble of Decision Trees trained with bagging and feature randomness.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEhbCyeWwd11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques.\n",
        "- Bagging\n",
        "\n",
        "- Boosting\n",
        "\n",
        "- Stacking"
      ],
      "metadata": {
        "id": "M6ZBVpJ-wd5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What is ensemble learning in machine learning\n",
        "- Technique of combining predictions of multiple models to improve performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "WhFOQzEowd7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  When should we avoid using ensemble methods\n",
        "- When interpretability is critical or computational resources are limited."
      ],
      "metadata": {
        "id": "xJYlhSAuwd-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting\n",
        "- By averaging predictions of multiple models trained on different bootstrap samples, reducing variance."
      ],
      "metadata": {
        "id": "Z2Pei66dweB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\n",
        "- It reduces overfitting and variance, leading to better generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "OwYYEGolyp3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging\n",
        "- Provides varied training data to individual models, improving robustness.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpzoaV5SyqEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What are some real-world applications of ensemble techniques\n",
        "-  Fraud detection, medical diagnosis, stock market prediction, recommendation systems, etc."
      ],
      "metadata": {
        "id": "EuuJXgxAyqJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting\n",
        "- Bagging: Parallel, reduces variance.\n",
        "\n",
        "- Boosting: Sequential, reduces bias and variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "G0By9NNIyqNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "SZAEoDWgyqQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "- from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "-  Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "- Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "- Build Bagging Classifier\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "- Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn-cFZN5yqTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)2.\n",
        "- from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "- X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "- model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- y_pred = model.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "449VHwP6yqWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "- from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "- X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "- model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- importances = model.feature_importances_\n",
        "for i, imp in enumerate(importances):\n",
        "    print(f\"Feature {i}: Importance {imp}\")\n"
      ],
      "metadata": {
        "id": "fXV-faBcz2gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "- from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "- Example: Diabetes dataset\n",
        "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "- model_rf.fit(X_train, y_train)\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "- y_pred_rf = model_rf.predict(X_test)\n",
        "y_pred_dt = model_dt.predict(X_test)\n",
        "\n",
        "- print(\"RF MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
        "print(\"DT MSE:\", mean_squared_error(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "0QpjbncLz2oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "- model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- print(\"OOB Score:\", model.oob_score_)\n"
      ],
      "metadata": {
        "id": "HLhlwt_Kz2uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "- from sklearn.svm import SVC\n",
        "\n",
        "- model = BaggingClassifier(base_estimator=SVC(probability=True), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "MTT6R0SGz21U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "- for n in [10, 50, 100, 200]:\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    print(f\"Trees: {n}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "id": "uSCepqSvz25l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "- from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "- model = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(\"AUC Score:\", auc)\n"
      ],
      "metadata": {
        "id": "qV53Uw1mz2_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores2.\n",
        "- model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "- importances = model.feature_importances_\n",
        "for i, imp in enumerate(importances):\n",
        "    print(f\"Feature {i}: Importance {imp}\")\n"
      ],
      "metadata": {
        "id": "4nLJHvDvz3D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "- Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "acc_rf = rf.score(X_test, y_test)\n",
        "\n",
        "- Bagging\n",
        "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "acc_bag = bag.score(X_test, y_test)\n",
        "\n",
        "- print(\"Random Forest Accuracy:\", acc_rf)\n",
        "print(\"Bagging Accuracy:\", acc_bag)\n"
      ],
      "metadata": {
        "id": "81UevzI0z3Ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.  Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
        "- from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "- param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "}\n",
        "\n",
        "- grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "yNvMByAI1udg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "- for n in [10, 50, 100, 200]:\n",
        "    model = BaggingRegressor(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"n_estimators: {n}, MSE: {mean_squared_error(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "-_Aikzol1ukS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples.\n",
        "- import numpy as np\n",
        "\n",
        "- y_pred = rf.predict(X_test)\n",
        "misclassified = np.where(y_test != y_pred)[0]\n",
        "print(\"Misclassified sample indices:\", misclassified)\n"
      ],
      "metadata": {
        "id": "iZljeXBF1uoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "- dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "acc_dt = dt.score(X_test, y_test)\n",
        "\n",
        "- bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "acc_bag = bag.score(X_test, y_test)\n",
        "\n",
        "- print(\"DT Accuracy:\", acc_dt)\n",
        "print(\"Bagging Accuracy:\", acc_bag)\n"
      ],
      "metadata": {
        "id": "67BBVSgQ1urE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "- from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "- ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "T--4V-SA1ut_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "- from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "- estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "- stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "- stack_model.fit(X_train, y_train)\n",
        "print(\"Stacking Classifier Accuracy:\", stack_model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "FoRy1CpB1uxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features.\n",
        "- import numpy as np\n",
        "\n",
        "- importances = rf.feature_importances_\n",
        "- indices = np.argsort(importances)[::-1]\n",
        "\n",
        "- for i in range(5):\n",
        "- print(f\"Feature {indices[i]}: Importance {importances[indices[i]]}\")\n"
      ],
      "metadata": {
        "id": "FT6NKbZE1uz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "- from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "- y_pred = bag.predict(X_test)\n",
        "\n",
        "- print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "- print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "- print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "axg58UIl1u3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "- for depth in [None, 5, 10, 15, 20]:\n",
        "    model = RandomForestClassifier(max_depth=depth, n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    print(f\"max_depth: {depth}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "id": "qrf1McMw1u7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance.\n",
        "- from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "-  DecisionTree Regressor\n",
        "dt_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_mse = mean_squared_error(y_test, dt_model.predict(X_test))\n",
        "\n",
        "-  KNeighbors Regressor\n",
        "kn_model = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "kn_model.fit(X_train, y_train)\n",
        "kn_mse = mean_squared_error(y_test, kn_model.predict(X_test))\n",
        "\n",
        "- print(\"DecisionTree Regressor MSE:\", dt_mse)\n",
        "- print(\"KNeighbors Regressor MSE:\", kn_mse)\n"
      ],
      "metadata": {
        "id": "xSIALcSb1u-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "- from sklearn.metrics import roc_auc_score\n",
        "\n",
        "- y_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
        " roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "- print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "id": "IJEVARRt1vDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "- from sklearn.model_selection import cross_val_score\n",
        "\n",
        "- scores = cross_val_score(bag, X_train, y_train, cv=5)\n",
        "- print(\"Cross-validation scores:\", scores)\n",
        "- print(\"Mean CV accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "cvLBwl6k1vGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "- from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "- PrecisionRecallDisplay.from_estimator(rf, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "uDslqlSg1vJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "- estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "- stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "- stack_model.fit(X_train, y_train)\n",
        "- print(\"Stacking Classifier Accuracy:\", stack_model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "S75R2TLS52iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.  Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "- for bootstrap_frac in [0.5, 0.7, 0.9, 1.0]:\n",
        "    model = BaggingRegressor(n_estimators=100, max_samples=bootstrap_frac, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "    print(f\"Bootstrap frac: {bootstrap_frac}, MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "NsxFQ0rH52ru"
      }
    }
  ]
}